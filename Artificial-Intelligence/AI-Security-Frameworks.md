
LLM Attack Surface; There are six key LLM (Large Language Model) components that can be targeted by attackers:
- Prompt - Attacks like prompt injections, where malicious input is used to manipulate the AI's output
- Response - Misuse or leakage of sensitive information in AI-generated responses
- Model - Theft, poisoning, or manipulation of the AI model
- Training Data - Introducing malicious data to alter the behavior of the AI.
- Infrastructure - Targeting the servers and services that support the AI
- Users - Misleading or exploiting the humans or systems relying on AI outputs

Frameworks
- [NIST Artificial Intelligence Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework)
- [Google's Secure AI Framework](https://safety.google/cybersecurity-advancements/saif/) - Follow link for page to download pdf
- [OWASP Top 10 for LLM Applications](https://genai.owasp.org/llm-top-10/) 
- [MITRE ATLAS](https://atlas.mitre.org/matrices/ATLAS)


## References

[thehackernews.com - 10/2024 from-misuse-to-abuse-ai-risks](https://thehackernews.com/2024/10/from-misuse-to-abuse-ai-risks-and.html)
